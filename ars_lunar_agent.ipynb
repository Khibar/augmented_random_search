{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARS Agent\n",
    "class ARSLunar():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"LunarLander-v2\")\n",
    "        self.epochs = 1000\n",
    "        self.episodes = 1000\n",
    "        self.alpha = 0.02\n",
    "        self.noise = 0.03\n",
    "        self.num = np.zeros(8)\n",
    "        self.mean = np.zeros(8)\n",
    "        self.diff = np.zeros(8)\n",
    "        self.var = np.zeros(8)\n",
    "        self.policy = np.zeros((4, 8))\n",
    "\n",
    "    def _track(self, _val):\n",
    "        self.num += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (_val - self.mean) / self.num\n",
    "        self.diff += (_val - last_mean) * (_val - self.mean)\n",
    "        self.var = (self.diff / self.num).clip(min=1e-2)\n",
    "\n",
    "    def _normalize(self, _params):\n",
    "        mean = self.mean\n",
    "        std = np.sqrt(self.var)\n",
    "        return (_params - mean) / std\n",
    "\n",
    "    def _evaluate(self, _params, _deltas=1, _trend=0):           \n",
    "        if _trend == 1:\n",
    "            return (self.policy + self.noise * _deltas).dot(_params)\n",
    "\n",
    "        if _trend == -1:\n",
    "            return (self.policy - self.noise * _deltas).dot(_params)\n",
    "        \n",
    "        return self.policy.dot(_params)\n",
    "\n",
    "    def _update(self, _updates, _reward_std):\n",
    "        step = np.zeros(self.policy.shape)\n",
    "        for p, n, d in _updates:\n",
    "            step += (p - n) * d\n",
    "\n",
    "        self.policy += self.alpha / (10 * _reward_std) * step\n",
    "\n",
    "    def _explore(self, _deltas=1, _trend=0):\n",
    "        state = self.env.reset()\n",
    "        end = False\n",
    "        eps = 0.\n",
    "        rv = 0\n",
    "\n",
    "        while not end and eps < self.episodes:\n",
    "            self._track(state)\n",
    "            state = self._normalize(state)\n",
    "            action = np.argmax(self._evaluate(state, _deltas, _trend))\n",
    "            state, reward, end, _ = self.env.step(action)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            rv += reward\n",
    "            eps += 1\n",
    "\n",
    "        return rv\n",
    "    \n",
    "    def _random_search(self):\n",
    "        deltas = [np.random.randn(*self.policy.shape) for _ in range(15)]\n",
    "        pos = [0] * 15\n",
    "        neg = [0] * 15\n",
    "\n",
    "        for idx in range(15):\n",
    "            pos[idx] = self._explore(deltas[idx], 1)\n",
    "\n",
    "        for idx in range(15):\n",
    "            neg[idx] = self._explore(deltas[idx], -1)\n",
    "\n",
    "        rewards = np.array(pos + neg)\n",
    "        reward_std = rewards.std()\n",
    "\n",
    "        res_table = {k: max(r_pos, r_neg) for k, (r_pos, r_neg) in enumerate(zip(pos, neg))}\n",
    "        sorted_res_table = sorted(res_table.keys(), key=lambda k: res_table[k], reverse=True)[:10]\n",
    "        updates = [(pos[k], neg[k], deltas[k]) for k in sorted_res_table]\n",
    "\n",
    "        self._update(updates, reward_std)\n",
    "        rv = self._explore()\n",
    "\n",
    "        return rv\n",
    "\n",
    "    def run(self):\n",
    "        start = time.time()\n",
    "        rv = []\n",
    "\n",
    "        for itr in range(self.epochs):\n",
    "            rewards = self._random_search()\n",
    "            time_stamp =  time.time() - start\n",
    "           \n",
    "            rv.append((itr, rewards, time_stamp))\n",
    "\n",
    "            if itr % 10 == 0: \n",
    "                print('Iteration:', itr, 'Rewards:', rewards)\n",
    "    \n",
    "        return rv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Agent\n",
    "ars_agent = ARSLunar()\n",
    "data = ars_agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data\n",
    "data = {\"400Epochs\": data[:400], \"allEpochs\": data}\n",
    "with open('analytics_data.json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('analytics_data.json') as fp:\n",
    "    json_data = json.load(fp)\n",
    "\n",
    "def plot_curve_iter(_data, _desc):\n",
    "    x = [v[1] for v in _data]\n",
    "    y = [v[0] for v in _data]\n",
    "\n",
    "    # Plot X and Y\n",
    "    plt.plot(y, x)\n",
    "    plt.title(_desc)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.show()\n",
    "\n",
    "plot_curve_iter(json_data[\"400Epochs\"], \"400 Iterations\")\n",
    "plot_curve_iter(json_data[\"allEpochs\"], \"1000 Iterations\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
